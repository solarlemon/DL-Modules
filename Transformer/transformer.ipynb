{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于word embedding\n",
    "以序列建模为例，考虑source setence和target sentence\n",
    "构建序列，序列的字符以其在词表中的索引的形式表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "# 单词表大小\n",
    "max_num_src_words=8\n",
    "max_num_tgt_words=8\n",
    "model_dim=8\n",
    "\n",
    "# 序列最大长度\n",
    "max_src_seq_len=5\n",
    "max_tgt_seq_len=5\n",
    "# src_len=torch.randint(2,5,(batch_size,))\n",
    "# tgt_len=torch.randint(2,5,(batch_size,))\n",
    "# batch_size=2，建立两个长度为2和4的源句子\n",
    "src_len = torch.Tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4,3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成的句子\n",
    "# src_seq=[torch.randint(1,max_num_src_words,(L,)) for L in src_len]\n",
    "# tgt_seq=[torch.randint(1,max_num_tgt_words,(L,)) for L in src_len]\n",
    "# [tensor([6, 6]), tensor([4, 7, 3, 5])] [tensor([2, 3]), tensor([3, 7, 6, 5])]\n",
    "# print(src_seq,tgt_seq) 在tensor([6,6])中，长度为2,单词索引为6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.将句子（序列）长度pad为最大长度，默认值为0     \n",
    "2.torch.unsqueeze()将序列扩成2维(第0维) 变成1*4  \n",
    "3.torch.cat()连接两个序列     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_seq=torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max_src_seq_len-L)),0) for L in src_len])\n",
    "tgt_seq=torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_tgt_words,(L,)),(0,max_tgt_seq_len-L)),0) for L in src_len])\n",
    "# print(src_seq)#tensor([[3, 2, 0, 0, 0],[7, 6, 1, 3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造embedding  \n",
    "`nn.Embedding(num_embeddings,embedding_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面已经定义model_dim=8\n",
    "src_embedding_table=nn.Embedding(max_num_src_words+1,model_dim) # +1是因为有个padding\n",
    "tgt_embedding_table=nn.Embedding(max_num_tgt_words+1,model_dim)\n",
    "\n",
    "print(src_embedding_table.weight)\n",
    "# 每一行是一个embedding向量 一行表示一个单词 第0行是pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_embedding=src_embedding_table(src_seq)\n",
    "tgt_embedding=tgt_embedding_table(src_seq)\n",
    "print(src_seq)\n",
    "print(src_embedding)\n",
    "# src_seq[[2, 5, 0, 0, 0],[6, 4, 2, 7, 0]]中表示两个句子。\n",
    "# 其中的两个向量表示单词索引，例如[2,5,0,0,0]对应的src_embedding则从src_embedding_table查表得到对应的值\n",
    "# 2取src_embedding_table的第2行（0行开始）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于position embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_position_len=5\n",
    "# 构建矩阵\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
